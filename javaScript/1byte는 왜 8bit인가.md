# 1byte는 왜 1bit 인가?



1byte는 컴퓨터가 처리하는 정보의 최소단위.

현대의 대부분의 컴퓨터는 8비트를 1바이트로 삼는다. 표준 C언어에서는 8비트 이상을 1바이트로 삼도록 규정하고 있다.

컴퓨터 아키텍쳐가 영문권인 곳에서 발전했기 때문에 굳이 8비트를 1바이트로 사용한다.

이진수로 이루어진 전자신호를 사람이 인식할 수 있는 문자로 저장해야했는데, 이런 문자를 표현하는 코드들의 숫자가 7~8bit로 충분했다.

​	ASCII code 

​	제어문자 32개, 출력가능문자 - 영소/대문자, 숫자, 기타 기호, parity bit

1개당 2개의 정보를 표시 가능한 비트를 8개 묶은 1옥텟으로 2의 8제곱, 즉 256 종류의 정보를 나타낼 수 있어 숫자와 영문자[[1\]](https://namu.wiki/w/바이트#fn-1)를 모두 표현할 수 있고 남는 공간에 특수문자까지 할당할 수 있다. 7비트보다 다루기 편하기 때문에 1옥텟=1바이트인 CPU가 나온 이래로 1옥텟이 사실상 표준 1바이트로 컴퓨터 세상을 평정했다. 사실, 7비트, 즉 128개만으로도 숫자와 영문자를 다 표현하고도 남는다. 실제로 [아스키 코드](https://namu.wiki/w/아스키 코드)는 7비트고, 이에 착안해서 실제로 7비트로 1바이트를 삼기도 한다.

컴퓨터가 이진의 수만 처리하던 시대엔 바이트라는 것은 별다른 의미가 없었을지도 모른다. 하지만 컴퓨터가 점점 대중화되고, 많은 목적을 가지게 되면서 당대 충분히 많은 문자를 표현할 수 있을 8비트를 1바이트(Bite)로, 그리고 Bit와 Bite와의 혼란을 피하기 위해 바이트(Byte)로, 그리고 추후 표준화를 위해 8비트로 쓰기 시작했다는 것을 대강 파악할 수 있었다.



유니코드의 목적은 현존하는 [문자 인코딩](https://ko.wikipedia.org/wiki/문자_인코딩) 방법들을 모두 유니코드로 교체하려는 것이다. 기존의 인코딩들은 그 규모나 범위 면에서 한정되어 있고, 다국어 환경에서는 서로 호환되지 않는 문제점이 있었다. 유니코드가 다양한 문자 집합들을 통합하는 데 성공하면서 유니코드는 [컴퓨터 소프트웨어](https://ko.wikipedia.org/wiki/컴퓨터_소프트웨어)의 [국제화와 지역화](https://ko.wikipedia.org/wiki/국제화와_지역화)에 널리 사용되게 되었으며, 비교적 최근의 기술인 [XML](https://ko.wikipedia.org/wiki/XML), [자바](https://ko.wikipedia.org/wiki/자바_(프로그래밍_언어)), 그리고 최신 [운영 체제](https://ko.wikipedia.org/wiki/운영_체제) 등에서도 지원하고 있다.

